{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqiZWNasr5Kj",
        "outputId": "9ff362e3-f63c-45b8-93c5-8f340aab7063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Setting up google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import other dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "# import my_utils as mu\n",
        "\n",
        "import urllib.request\n",
        "from os.path import isfile, isdir\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "z762wDUJs3xl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Read dataset and create dataloaders"
      ],
      "metadata": {
        "id": "VoXfx03eslo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Dataset\n",
        "\n",
        "\n",
        "# Define the CIFAR-10 dataset URL and folder path\n",
        "cifar_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# check if the data file is already downloaded\n",
        "# if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "# and save as cifar-10-python.tar.gz\n",
        "if not isfile('cifar-10.tar.gz'):\n",
        "  # Download the dataset\n",
        "  print(\"Downloading CIFAR-10 dataset...\")\n",
        "  urllib.request.urlretrieve(cifar_url, \"cifar-10.tar.gz\")\n",
        "  print(\"Download complete!\")\n",
        "else:\n",
        "  print(\"Already downloaded\")\n",
        "\n",
        "# Extract the downloaded archive\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "  print(\"Extracting CIFAR-10...\")\n",
        "  with tarfile.open(\"cifar-10.tar.gz\", \"r:gz\") as tar_file:\n",
        "    tar_file.extractall()\n",
        "  print(\"Extraction complete!\")\n",
        "else:\n",
        "  print(\"Already extracted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4XrRk-LzJgO",
        "outputId": "ba2caa9e-af41-44a3-8390-74e34c454c52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading CIFAR-10 dataset...\n",
            "Download complete!\n",
            "Extracting CIFAR-10...\n",
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Data loaders\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define data directory\n",
        "data_dir = \"./\"\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load train and test datasets\n",
        "train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=False, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=data_dir, train=False, download=False, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "MizfLLya2q62"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - Create the Model"
      ],
      "metadata": {
        "id": "ccbf6CG6swzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, num_inputs, k):\n",
        "    super(Block, self).__init__()\n",
        "    self.k = k\n",
        "    self.spatialAveragePool = nn.AdaptiveAvgPool2d(output_size=(k, k))\n",
        "    self.W = nn.Linear(k, num_inputs)\n",
        "    self.conv_layers = nn.ModuleList([nn.Conv2d(3, 3, 1, 1) for _ in range(k)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    a = nn.functional.relu(self.W(self.spatialAveragePool(x)))\n",
        "    o = torch.sum(torch.stack([conv(x) for conv in self.conv_layers]), dim=0)\n",
        "    return o\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Backbone, self).__init__()\n",
        "\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(3, 16),\n",
        "        # Block(num_inputs[i], num_outputs[i]),\n",
        "        # Block(num_inputs[i], num_outputs[i])\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.blocks(x)\n",
        "    return out\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, num_inputs, num_classes):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.num_inputs = num_inputs\n",
        "\n",
        "    # self.Fltn = nn.Flatten()\n",
        "\n",
        "    # softmax regression\n",
        "    self.Linear1 = nn.Linear(num_inputs, num_classes)\n",
        "    nn.init.normal_(self.Linear1.weight, std=0.01)\n",
        "    nn.init.zeros_(self.Linear1.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x = self.Fltn(x)\n",
        "\n",
        "    x = x.view(-1, 3*32*32)\n",
        "    x = self.Linear1(x)\n",
        "    return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.backbone = Backbone()\n",
        "    self.classifier = Classifier(3*32*32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.backbone(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "GA2VXJ7fsem7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()"
      ],
      "metadata": {
        "id": "G8xaQg4p0rQE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Create the Loss and Optimizer"
      ],
      "metadata": {
        "id": "V5YxVQnetM8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)"
      ],
      "metadata": {
        "id": "NiZB2Em2shF2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4 - Write the training script"
      ],
      "metadata": {
        "id": "EE2tOf9Vtx-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    # Get inputs and labels\n",
        "    inputs, labels = data\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    l = loss(outputs, labels)\n",
        "    l.backward()\n",
        "\n",
        "    # Update optimizer weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print statistics\n",
        "    running_loss += l.item()\n",
        "    if i % 100 == 0:  # Print every 100 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch + 1, i, running_loss / 100))\n",
        "    running_loss = 0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqPA8kUFt40S",
        "outputId": "aba6fec4-e2c3-4596-f894-0f47ba1bdf63"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     0] loss: 0.025\n",
            "[1,   100] loss: 0.022\n",
            "[1,   200] loss: 0.023\n",
            "[1,   300] loss: 0.023\n",
            "[1,   400] loss: 0.032\n",
            "[1,   500] loss: 0.698\n",
            "[1,   600] loss: 0.067\n",
            "[1,   700] loss: 0.022\n",
            "[1,   800] loss: 0.024\n",
            "[1,   900] loss: 0.021\n",
            "[1,  1000] loss: 0.023\n",
            "[1,  1100] loss: 0.024\n",
            "[1,  1200] loss: 0.022\n",
            "[1,  1300] loss: 0.022\n",
            "[1,  1400] loss: 0.031\n",
            "[1,  1500] loss: 0.035\n",
            "[2,     0] loss: 0.030\n",
            "[2,   100] loss: 0.034\n",
            "[2,   200] loss: 0.037\n",
            "[2,   300] loss: 0.028\n",
            "[2,   400] loss: 0.658\n",
            "[2,   500] loss: 0.036\n",
            "[2,   600] loss: 0.028\n",
            "[2,   700] loss: 0.023\n",
            "[2,   800] loss: 0.024\n",
            "[2,   900] loss: 0.023\n",
            "[2,  1000] loss: 0.016\n",
            "[2,  1100] loss: 0.022\n",
            "[2,  1200] loss: 0.026\n",
            "[2,  1300] loss: 0.021\n",
            "[2,  1400] loss: 0.029\n",
            "[2,  1500] loss: 0.031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5 - Final Model Accuracy on the Validation Set"
      ],
      "metadata": {
        "id": "b_t2TVAB3hz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "id": "EFO_gSSZ3rpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cc69bc-5896-44ef-cc3f-fffb4b58a62b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 30 %\n"
          ]
        }
      ]
    }
  ]
}